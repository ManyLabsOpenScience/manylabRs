---
title: "ManyLabs2 Data Analysis"
author: "[ManyLabs2](https://osf.io/8cd4r) (Corresponding coder: [Fred Hasselman](https://osf.io/ujgs6/))"
date: "24 October 2015"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data analysis how-to}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(tidy=FALSE)
require(manylabRs)
library(reshape2)
library(plyr)
library(broom)
library(tidyverse)
```

# Analysis Strategy

The ManyLabs2 data analysis strategy attempts to regard three principles that maximize research transparency:
  
 1. **Principle of Equality**: All data should be treated equally by a code. That is, the code should do its job generating results while at the same time being as naive as possible to the particular facts of the study being analysed. This will reduce any chances of bias with respect to the outcomes of a certain dataset or a particular study. If it is necessary to add study specific code, the second principle should be regarded.
 
 2. **Principle of Transparency**: All operations that are crucial for obtaining an analysis result should be available for inspection by anyone who wishes to do so. This should be possible without the help of the auhtors that generated the code. The operations concern the application of data filtering rules, computation of variables derived from original measurements, running an analysis and constructing graphs, tables and figures. If full transparency is not possible, the third principle should be regarded. 
 
 3. **Principle of Reproducibility**: The most basic requirement for analysis results is that they should be reproducable given the original code and the original data set. However, any new implementation of the same analysis strategy in a different context, or application of the code to a different dataset, e.g. a replication study, should not be problematic. That is, outcomes may differ between data sets, but this should not be attributable to any details of the code or the analysis strategy.  
    
    
## `R` as a parser of online code.    

The [pre-registered Manylabs2 protocol](https://docs.google.com/document/d/1B5sVz3jlKMGnpxik-cfCeR8ngeQ249bzQExaVd5FzZM/edit) describes a number of analyses per replication study that can be categorised as **Primary** (target replications per site), **Secondary** (additional analyses per site, e.g. on subgroups), and **Global** (analyses on the entire dataset). 

These *promised analyses* have all been implemented in `R` in a transparent way and this implementation is now ready for an independent review.    

## Implementation

Functions avalaible in an [`R` package on GitHub](https://github.com/ManyLabsOpenScience/manylabRs) ([PDF manual](https://github.com/ManyLabsOpenScience/manylabRs/blob/master/inst/manylabRs.pdf)) extract information and instructions about each promised analysis a table that is openly accessible, the [masteRkey spreadsheet](https://docs.google.com/spreadsheets/d/1fqK3WHwFPMIjNVVvmxpMEjzUETftq_DmP5LzEhXxUHA/edit#gid=769239110).

Each row in the table represents an analysis, the columns contain specific information about the analysis:   

* Columns A through E are identifiers for study, analysis and slate.

* Column F and G contain `R` commands which will extract and label the columnms from the dataset needed for the analysis.

* Column H and I contain filter instructions for cases and subsamnples.

* Columns J through L contain information about the nature of the analysis (Global, Primary Secondary).

* Columns M through O contain information for study inclusion in figures and tables

* Column P lists the name of a analysis specific *variable function* (`varfun.`) which in most cases just reorganises the variables specified in previous columns so they can be passed to the analysis code. In some cases these function perform specific calculations required by the original analyses.

* Columns Q through V contain information about the statistiscal tests 

The `R` package [**manylabRs**](https://github.com/ManyLabsOpenScience/manylabRs) contains the `R` functions that can read the information from this sheet and conduct analyses on the data.


## Install the package

Several ways to install the package.

### Source from GitHub

Use the code below to install the `manylabRs` package directly from [GitHub](https://github.com/ManyLabsOpenScience/manylabRs).

```{r, eval=FALSE}
library(devtools)
install_github("ManyLabsOpenScience/manylabRs")
library(manylabRs)
library(tidyverse)
library(broom)
```

### Download tarball from GitHub

First [download the tarball](https://github.com/ManyLabsOpenScience/manylabRs/pkg/), then install the package locally through the RStudio package installer: `Tools` >> `Install Packages...`


## Main function: `get.analyses()`

The main function to inspect is `get.analyses()`.  

It will take one or more take analysis (`studies`) from the `masteRkey` sheet and an indication of whether the analysis is:
1. `global` - will disregard the clusters in the data and use all valid caes for analyses, both `primary` and `secondary` analyses have a `global` variant.
2. `primary`- target analysis of replication study conducted for each lab seperately.
3. `secondary` - additional analyses for each lab conducted for each lab seperately.
4. `order` - presentation order analyses disregard the clusters int he data, each order is analysed seperately

> Have a look at [`saveConsole.R`](https://github.com/ManyLabsOpenScience/manylabRs/blob/master/inst/saveConsole.R) which calls the `testScript()` function and creates a log file with lots of info about the analysis steps.


**IMPORTANT FOR REVIEWERS**

You will have to point the function`get.analyses()` to where you downloaded these files:     

`ML2_RawData_S1.rds`    
`ML2_RawData_S2.rds`     

The script will assume the data are in a subdirectory of a rootdirectory given by the arguments:
```{r echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
rootdir = "YOUR PATHNAME TO DATAFILES ROOTDIR"

indir = list(RAW.DATA   = "DIRECTORY IN YOUR rootdir CONTAINING DATAFILES",
             MASTERKEY  = "", 
             SOURCEINFO = "")
```


```{r, echo=FALSE}
MyRootDir    <- "~/Documents/GitHub/manylabRs/"
MyRawDataDir <- "random3rd"
```


The example below runs a global analysis for `Huang.1`
```{r}
df <- get.analyses(studies       = 1, 
                   analysis.type = 1, 
                   rootdir       = MyRootDir,
                   indir = list(RAW.DATA   = MyRawDataDir, 
                                MASTERKEY  = "", 
                                SOURCEINFO = ""))
```

The object `df` contains two named lists:^[the names correspond to the analysis name in the `masteRkey`]

### `raw.case`

This list contains dataframes with the relevant variables for each analysis, but before the analysis specific variable functions (`vafun`) are applied. There is a Boolean variable `case.include` which indicates whther a case is valid and should be included for analysis.

```{r}
head(tbl_df(df$raw.case$Huang.1))
```

### `aggregated`    

The dataframe in `aggregated` contains the data as is was analysed, after the `varfun` is applied.

```{r}
glimpse(tbl_df(df$aggregated$Huang.1))
```


The output contains descriptives and sample summary characteristics and a variety of Effect Size measures.
It also contains the console output of the statistical test that was conducted:

```{r}
cat(paste0(df$aggregated$Huang.1$test.ConsoleOutput))
```

## **A closer look: Four steps to get from data to results**

The code of the function `get.analyses()` is not very readable because it include a lot of error-checking, error-reporting, conditional statements and... well... because we are not professional software engineers, but scientist doing the best we can.     

The function was created to be able to handle a batch of many different analyses in one go and save the results many different files. Here, we will skip the error-checks and file-saving and focus on the four major steps by taking the first analysis in the `masteRkey` spreadsheet, `Huang.1`, and analysing the data from the `source` named `brasilia`.

The four steps that are be applied to all analyses in ManyLabs 2 are:

1. **C**ollect information about the analysis from the `masteRkey` spreadsheet.
2. **E**xtract the data and reorganise it so it can be passed to the analysis function
3. **C**onduct the analysis
4. **E**xtract the results


### Step 1 - Collect information

The first step is always to use the `masteRkey` spreadsheet to gather all the information needed to:
* Extract the appropriate data (variables and cases) for this analysis and `source`
* Conduct the appropriate analyis on the extracted data

```{r step1, echo=TRUE, tidy=FALSE}
# NOTE: This example follows some (but not all!!) steps of the main function: get.analyses()

# Select analysis 1 [Huang.1] and source 'brasilia'
runningAnalysis <- 1
runningGroup    <- 'brasilia'

# Get information about the analysis to run
masteRkeyInfo  <- get.GoogleSheet(data='ML2masteRkey')$df[runningAnalysis,]

# Get the appropriate 'raw' dataset [Slate1 or Slate2]
ifelse(masteRkeyInfo$study.slate == 1, data(ML2_S1),data(ML2_S2))

# Organise the information into a list object
analysisInfo   <- get.info(masteRkeyInfo, colnames(ML2.S1), subset="all")

# Use analysisInfo to generate a cahin of filter instructions to select valid variables and cases
filterChain <- get.chain(analysisInfo)
```

Let's have a look at the filterChain object:
```{r}
filterChain
```

It contains two fields:
* `$df` a `dplyr` command for selecting the appropriate variables and (if applicable) filtering on `source` characteristics indicated by the column `masteRkey$source.include` (e.g. whether the data were collected on-line). For the present analysis we do not have to filter on `source` characteristics.
* `$vars`  selecting valid rows.

### Step 2 - Extract the data

```{r step 2}
# Apply the filterChain to select aprropriate variables from ML2.S1
df.raw <- eval(parse(text=paste("ML2.S1", filterChain$df)))

# Apply the filterChain to generate a list object that represents the design cells
df.split <- get.sourceData(filterChain, df.raw[df.raw$source%in%runningGroup,], analysisInfo)

# Create a list object with data vectors and appropriate labels, that can be passed to the analysis function
vars   <- eval(parse(text=paste0(masteRkeyInfo$stat.vars,'(df.split)',collapse="")))
```

### Step 3 - Conduct the analysis


```{r step 3}
# Get the paramers the parameters to use for the statistical analysis
stat.params <<- analysisInfo$stat.params

# Run the analysis listed in masteRkey column 'stat.test' usinf the data vectors in 'vars'
stat.test   <- with(vars, eval(parse(text = masteRkeyInfo$stat.test)))
```

### Step 4 - Extract the results


```{r step 4}
# Return descriptives and summaries
describe <- get.descriptives(stat.test = stat.test, vars = vars, keytable  = masteRkeyInfo)

# Generate output
ESCI <- generateOutput(describe = describe, runningGroup = runningGroup, runningAnalysis = runningAnalysis)
```

## Other algorithms

+ Get information from `masteRkey` on the analyses to run: `get.info()`
+ Get a data filter based on exclusion criteria: `get.chain()`
+ Select the appropriate variables: `get.sourceData()`
+ Apply the analysis-specific variable function: `varfun.ABC.#()`
+ Apply the analysis listed in column `stat.test` to the data
+ Organise the output `get.descriptives()`
+ Calculate confidence intervals for effect sizes `any2any()`
+ Return the ouput





